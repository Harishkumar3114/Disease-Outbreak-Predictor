{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c357c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "RAW_PATH = r\"C:\\Users\\Deepak\\OneDrive\\Desktop\\ObjectRegon\\data\\raw\"\n",
    "PROCESSED_PATH = r\"C:\\Users\\Deepak\\OneDrive\\Desktop\\ObjectRegon\\data\\processed\"\n",
    "\n",
    "os.makedirs(PROCESSED_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d5c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Downcast numerics safely (without changing schema across chunks).\"\"\"\n",
    "    for col in df.select_dtypes(include=[\"int\", \"float\"]).columns:\n",
    "        # always downcast to float32 to keep schema consistent\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bdb50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def process_csv(filename, chunksize=500_000):\n",
    "    input_path = os.path.join(RAW_PATH, filename)\n",
    "    output_path = os.path.join(PROCESSED_PATH, filename.replace(\".csv\", \".parquet\"))\n",
    "\n",
    "    # Infer dtypes from a small sample\n",
    "    sample = pd.read_csv(input_path, nrows=500)\n",
    "    sample_dtypes = sample.dtypes.to_dict()\n",
    "\n",
    "    # üîπ Convert all int columns ‚Üí float to handle NaN\n",
    "    for col, dtype in sample_dtypes.items():\n",
    "        if pd.api.types.is_integer_dtype(dtype):\n",
    "            sample_dtypes[col] = \"float64\"\n",
    "\n",
    "    writer = None\n",
    "\n",
    "    for chunk in pd.read_csv(input_path, chunksize=chunksize, dtype=sample_dtypes, low_memory=False):\n",
    "        chunk = optimize_dtypes(chunk)\n",
    "        table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(output_path, table.schema, compression=\"snappy\")\n",
    "\n",
    "        writer.write_table(table)\n",
    "\n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "    print(f\"‚úÖ Processed {filename} ‚Üí {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533adc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "RAW_PATH = r\"C:\\Users\\Deepak\\OneDrive\\Desktop\\ObjectRegon\\data\\raw\"\n",
    "OUTPUT_PATH = r\"C:\\Users\\Deepak\\OneDrive\\Desktop\\ObjectRegon\\data\\processed\"\n",
    "\n",
    "def clean_and_save_csv(filename, usecols=None, chunksize=500_000, to_parquet=True):\n",
    "    \"\"\"\n",
    "    Clean a large CSV in chunks and save optimized version.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): CSV file name inside RAW_PATH\n",
    "        usecols (list): columns to keep (None = keep all)\n",
    "        chunksize (int): number of rows per chunk\n",
    "        to_parquet (bool): save as parquet (True) or CSV (False)\n",
    "    \"\"\"\n",
    "    input_path = os.path.join(RAW_PATH, filename)\n",
    "    output_path = os.path.join(\n",
    "        OUTPUT_PATH, filename.replace(\".csv\", \".parquet\" if to_parquet else \"_clean.csv\")\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    for chunk in pd.read_csv(input_path, chunksize=chunksize, usecols=usecols, low_memory=False):\n",
    "        # üîπ Convert ints ‚Üí float32 (handles NaN safely)\n",
    "        for col in chunk.select_dtypes(include=[\"int\", \"float\"]).columns:\n",
    "            chunk[col] = chunk[col].astype(\"float32\")\n",
    "\n",
    "        # üîπ Drop fully empty columns\n",
    "        chunk = chunk.dropna(axis=1, how=\"all\")\n",
    "\n",
    "        all_chunks.append(chunk)\n",
    "\n",
    "    df = pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "    if to_parquet:\n",
    "        df.to_parquet(output_path, engine=\"pyarrow\", compression=\"snappy\", index=False)\n",
    "    else:\n",
    "        df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Cleaned {filename}, final shape: {df.shape} ‚Üí saved to {output_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e175bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "RAW_PATH = \"C:/Users/Deepak/OneDrive/Desktop/ObjectRegon/data/raw\"\n",
    "PROC_PATH = \"C:/Users/Deepak/OneDrive/Desktop/ObjectRegon/data/processed\"\n",
    "os.makedirs(PROC_PATH, exist_ok=True)\n",
    "\n",
    "def convert_to_parquet(filename, usecols=None, chunksize=200_000):\n",
    "    input_path = os.path.join(RAW_PATH, filename)\n",
    "    output_path = os.path.join(PROC_PATH, filename.replace(\".csv\", \".parquet\"))\n",
    "\n",
    "    all_chunks = []\n",
    "    for chunk in pd.read_csv(input_path, chunksize=chunksize, usecols=usecols, low_memory=False):\n",
    "        # Convert all numeric columns ‚Üí float32 (saves memory)\n",
    "        for col in chunk.select_dtypes(include=[\"int\", \"float\"]).columns:\n",
    "            chunk[col] = chunk[col].astype(\"float32\")\n",
    "        all_chunks.append(chunk)\n",
    "\n",
    "    df = pd.concat(all_chunks, ignore_index=True)\n",
    "    df.to_parquet(output_path, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "\n",
    "    print(f\"‚úÖ Converted {filename} ‚Üí {output_path}, final shape: {df.shape}\")\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f7a02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_columns = {\n",
    "    \"epidemiology.csv\": [\n",
    "        \"date\", \"location_key\", \"new_confirmed\", \"new_deceased\", \"cumulative_confirmed\", \"cumulative_deceased\"\n",
    "    ],\n",
    "    \"mobility.csv\": [\n",
    "        \"date\", \"location_key\", \"mobility_retail_and_recreation\", \"mobility_workplaces\"\n",
    "    ],\n",
    "    \"weather.csv\": [\n",
    "        \"date\", \"location_key\", \"average_temperature_celsius\", \"rainfall_mm\", \"relative_humidity\"\n",
    "    ],\n",
    "    \"hospitalizations.csv\": [\n",
    "        \"date\", \"location_key\", \"new_hospitalized_patients\", \"cumulative_hospitalized_patients\"\n",
    "    ],\n",
    "    \"vaccinations.csv\": [\n",
    "        \"date\", \"location_key\", \"new_persons_vaccinated\", \"cumulative_persons_vaccinated\"\n",
    "    ],\n",
    "    \"oxford-government-response.csv\": [\n",
    "        \"date\", \"location_key\", \"stringency_index\", \"school_closing\", \"workplace_closing\"\n",
    "    ],\n",
    "    \"google-search-trends.csv\": [\n",
    "        \"date\", \"location_key\", \"search_trends_covid\", \"search_trends_vaccine\"\n",
    "    ],\n",
    "    \"by-age.csv\": [\n",
    "        \"date\", \"location_key\", \"age_bin_0\", \"age_bin_1\", \"age_bin_2\", \"age_bin_3\", \"age_bin_4\", \"age_bin_5\",\n",
    "        \"age_bin_6\", \"age_bin_7\", \"age_bin_8\", \"age_bin_9\"\n",
    "    ],\n",
    "    \"by-sex.csv\": [\n",
    "        \"date\", \"location_key\", \"new_confirmed_male\", \"new_confirmed_female\", \"cumulative_confirmed_male\",\n",
    "        \"cumulative_confirmed_female\"\n",
    "    ],\n",
    "    \"index.csv\": [\n",
    "        \"location_key\", \"country_name\", \"subregion1_name\", \"subregion2_name\", \"locality_name\",\n",
    "        \"population\", \"gdp_per_capita\", \"human_development_index\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c83dca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Converted index.csv ‚Üí C:/Users/Deepak/OneDrive/Desktop/ObjectRegon/data/processed\\index.parquet, final shape: (22963, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:/Users/Deepak/OneDrive/Desktop/ObjectRegon/data/processed\\\\index.parquet'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index ‚Äì keep the most relevant columns for joins\n",
    "convert_to_parquet(\n",
    "    \"index.csv\",\n",
    "    usecols=[\n",
    "        \"location_key\",\n",
    "        \"country_code\",\n",
    "        \"country_name\",\n",
    "        \"subregion1_name\",\n",
    "        \"subregion2_name\",\n",
    "        \"locality_name\",\n",
    "        \"aggregation_level\"\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11fe5b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç index.csv columns detected: ['location_key', 'place_id', 'wikidata_id', 'datacommons_id', 'country_code', 'country_name', 'subregion1_code', 'subregion1_name', 'subregion2_code', 'subregion2_name', 'locality_code', 'locality_name', 'iso_3166_1_alpha_2', 'iso_3166_1_alpha_3', 'aggregation_level']\n",
      "‚úÖ Saved index.csv ‚Üí C:/Users/Deepak/OneDrive/Desktop/ObjectRegon/data/processed\\index.parquet, shape: (22963, 4)\n"
     ]
    }
   ],
   "source": [
    "# By Age ‚Äì safer version\n",
    "def convert_to_parquet_flexible(filename, try_usecols=None):\n",
    "    input_path = os.path.join(RAW_PATH, filename)\n",
    "    output_path = os.path.join(PROC_PATH, filename.replace(\".csv\", \".parquet\"))\n",
    "\n",
    "    # Peek at columns first\n",
    "    cols = pd.read_csv(input_path, nrows=5).columns.tolist()\n",
    "    print(f\"üîç {filename} columns detected:\", cols)\n",
    "\n",
    "    # Intersect requested usecols with actual columns\n",
    "    if try_usecols:\n",
    "        usecols = [c for c in try_usecols if c in cols]\n",
    "        if not usecols:\n",
    "            print(f\"‚ö†Ô∏è None of the requested columns found in {filename}, keeping all\")\n",
    "            usecols = None\n",
    "    else:\n",
    "        usecols = None\n",
    "\n",
    "    all_chunks = []\n",
    "    for chunk in pd.read_csv(input_path, chunksize=500_000, usecols=usecols, low_memory=False):\n",
    "        for col in chunk.select_dtypes(include=[\"int\", \"float\"]).columns:\n",
    "            chunk[col] = chunk[col].astype(\"float32\")\n",
    "        all_chunks.append(chunk)\n",
    "\n",
    "    df = pd.concat(all_chunks, ignore_index=True)\n",
    "    df.to_parquet(output_path, index=False, compression=\"snappy\")\n",
    "    print(f\"‚úÖ Saved {filename} ‚Üí {output_path}, shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Now run for by-age\n",
    "by_age = convert_to_parquet_flexible(\n",
    "    \"index.csv\",\n",
    "    try_usecols=[\n",
    "        \"location_key\",\n",
    "        \"country_code\",\n",
    "        \"country_name\",\n",
    "        \"subregion1_name\",\n",
    "        \"population\",\n",
    "        \"gdp_per_capita\",\n",
    "        \"human_development_index\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54dd6977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final merged shape: (12525825, 43)\n",
      "üìÇ Merged dataset saved to: C:/Users/Deepak/OneDrive/Desktop/ObjectRegon/data/processed\\merged_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# üìÇ Path where your parquet files are stored\n",
    "DATA_PATH = \"C:/Users/Deepak/OneDrive/Desktop/ObjectRegon/data/processed\"  # change if needed\n",
    "\n",
    "# üìå List of parquet files (preprocessed)\n",
    "files = {\n",
    "    \"epidemiology\": \"epidemiology.parquet\",\n",
    "    \"mobility\": \"mobility.parquet\",\n",
    "    \"weather\": \"weather.parquet\",\n",
    "    \"hospitalizations\": \"hospitalizations.parquet\",\n",
    "    \"vaccinations\": \"vaccinations.parquet\",\n",
    "    \"oxford\": \"oxford-government-response.parquet\",\n",
    "    \"google\": \"google-search-trends.parquet\",\n",
    "    \"by_age\": \"by-age.parquet\",\n",
    "    \"by_sex\": \"by-sex.parquet\",\n",
    "    \"index\": \"index.parquet\"\n",
    "}\n",
    "\n",
    "# üìå Load all parquet files\n",
    "datasets = {name: pd.read_parquet(os.path.join(DATA_PATH, fname)) for name, fname in files.items()}\n",
    "\n",
    "# üìå Start merging\n",
    "# Merge epidemiology first (it will be the base table)\n",
    "merged = datasets[\"epidemiology\"]\n",
    "\n",
    "# Merge all other datasets on ['date', 'location_key'] if they have date\n",
    "# Index dataset is static (location-level), so merge only on 'location_key'\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if name == \"epidemiology\":\n",
    "        continue\n",
    "    \n",
    "    if \"date\" in df.columns:\n",
    "        merged = pd.merge(merged, df, on=[\"date\", \"location_key\"], how=\"left\")\n",
    "    else:\n",
    "        merged = pd.merge(merged, df, on=\"location_key\", how=\"left\")\n",
    "\n",
    "print(\"‚úÖ Final merged shape:\", merged.shape)\n",
    "\n",
    "# üìå Save final merged dataset\n",
    "output_path = os.path.join(DATA_PATH, \"merged_dataset.parquet\")\n",
    "merged.to_parquet(output_path, index=False)\n",
    "\n",
    "print(\"üìÇ Merged dataset saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b98e353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date location_key  new_confirmed  new_deceased  cumulative_confirmed  \\\n",
      "0  2020-01-01           AD            0.0           0.0                   0.0   \n",
      "1  2020-01-02           AD            0.0           0.0                   0.0   \n",
      "2  2020-01-03           AD            0.0           0.0                   0.0   \n",
      "3  2020-01-04           AD            0.0           0.0                   0.0   \n",
      "4  2020-01-05           AD            0.0           0.0                   0.0   \n",
      "\n",
      "   mobility_retail_and_recreation  mobility_grocery_and_pharmacy  \\\n",
      "0                             NaN                            NaN   \n",
      "1                             NaN                            NaN   \n",
      "2                             NaN                            NaN   \n",
      "3                             NaN                            NaN   \n",
      "4                             NaN                            NaN   \n",
      "\n",
      "   mobility_transit_stations  mobility_workplaces  mobility_residential  ...  \\\n",
      "0                        NaN                  NaN                   NaN  ...   \n",
      "1                        NaN                  NaN                   NaN  ...   \n",
      "2                        NaN                  NaN                   NaN  ...   \n",
      "3                        NaN                  NaN                   NaN  ...   \n",
      "4                        NaN                  NaN                   NaN  ...   \n",
      "\n",
      "   relative_humidity  new_confirmed_male  new_confirmed_female  \\\n",
      "0          72.773048                 NaN                   NaN   \n",
      "1          70.841316                 NaN                   NaN   \n",
      "2          71.117249                 NaN                   NaN   \n",
      "3          77.338638                 NaN                   NaN   \n",
      "4          60.762379                 NaN                   NaN   \n",
      "\n",
      "   cumulative_confirmed_male  cumulative_confirmed_female  country_code  \\\n",
      "0                        NaN                          NaN            AD   \n",
      "1                        NaN                          NaN            AD   \n",
      "2                        NaN                          NaN            AD   \n",
      "3                        NaN                          NaN            AD   \n",
      "4                        NaN                          NaN            AD   \n",
      "\n",
      "   country_name subregion1_name subregion2_name aggregation_level  \n",
      "0       Andorra            None            None               0.0  \n",
      "1       Andorra            None            None               0.0  \n",
      "2       Andorra            None            None               0.0  \n",
      "3       Andorra            None            None               0.0  \n",
      "4       Andorra            None            None               0.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12525825 entries, 0 to 12525824\n",
      "Data columns (total 22 columns):\n",
      " #   Column                          Dtype  \n",
      "---  ------                          -----  \n",
      " 0   date                            object \n",
      " 1   location_key                    object \n",
      " 2   new_confirmed                   float32\n",
      " 3   new_deceased                    float32\n",
      " 4   cumulative_confirmed            float32\n",
      " 5   mobility_retail_and_recreation  float32\n",
      " 6   mobility_grocery_and_pharmacy   float32\n",
      " 7   mobility_transit_stations       float32\n",
      " 8   mobility_workplaces             float32\n",
      " 9   mobility_residential            float32\n",
      " 10  average_temperature_celsius     float32\n",
      " 11  rainfall_mm                     float32\n",
      " 12  relative_humidity               float32\n",
      " 13  new_confirmed_male              float32\n",
      " 14  new_confirmed_female            float32\n",
      " 15  cumulative_confirmed_male       float32\n",
      " 16  cumulative_confirmed_female     float32\n",
      " 17  country_code                    object \n",
      " 18  country_name                    object \n",
      " 19  subregion1_name                 object \n",
      " 20  subregion2_name                 object \n",
      " 21  aggregation_level               float32\n",
      "dtypes: float32(16), object(6)\n",
      "memory usage: 1.3+ GB\n",
      "None\n",
      "mobility_transit_stations         9863581\n",
      "mobility_grocery_and_pharmacy     9313984\n",
      "new_confirmed_female              9121992\n",
      "mobility_retail_and_recreation    9120728\n",
      "new_confirmed_male                9115504\n",
      "mobility_residential              9083750\n",
      "cumulative_confirmed_female       8867333\n",
      "cumulative_confirmed_male         8866646\n",
      "mobility_workplaces               7372317\n",
      "subregion2_name                   1038784\n",
      "new_deceased                       858687\n",
      "rainfall_mm                        307232\n",
      "relative_humidity                  255361\n",
      "average_temperature_celsius        252565\n",
      "subregion1_name                    228644\n",
      "cumulative_confirmed               198780\n",
      "new_confirmed                       50025\n",
      "location_key                          987\n",
      "country_code                          987\n",
      "date                                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"C:/Users/Deepak/OneDrive/Desktop/ObjectRegon/data/processed/merged_reduced.parquet\")\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.isna().sum().sort_values(ascending=False).head(20))  # check missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9530f1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced shape: (12525825, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>location_key</th>\n",
       "      <th>new_confirmed</th>\n",
       "      <th>new_deceased</th>\n",
       "      <th>cumulative_confirmed</th>\n",
       "      <th>mobility_retail_and_recreation</th>\n",
       "      <th>mobility_grocery_and_pharmacy</th>\n",
       "      <th>mobility_transit_stations</th>\n",
       "      <th>mobility_workplaces</th>\n",
       "      <th>mobility_residential</th>\n",
       "      <th>...</th>\n",
       "      <th>relative_humidity</th>\n",
       "      <th>new_confirmed_male</th>\n",
       "      <th>new_confirmed_female</th>\n",
       "      <th>cumulative_confirmed_male</th>\n",
       "      <th>cumulative_confirmed_female</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>subregion1_name</th>\n",
       "      <th>subregion2_name</th>\n",
       "      <th>aggregation_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>AD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>72.773048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AD</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>AD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>70.841316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AD</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>AD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>71.117249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AD</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>AD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>77.338638</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AD</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>AD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>60.762379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AD</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date location_key  new_confirmed  new_deceased  cumulative_confirmed  \\\n",
       "0  2020-01-01           AD            0.0           0.0                   0.0   \n",
       "1  2020-01-02           AD            0.0           0.0                   0.0   \n",
       "2  2020-01-03           AD            0.0           0.0                   0.0   \n",
       "3  2020-01-04           AD            0.0           0.0                   0.0   \n",
       "4  2020-01-05           AD            0.0           0.0                   0.0   \n",
       "\n",
       "   mobility_retail_and_recreation  mobility_grocery_and_pharmacy  \\\n",
       "0                             NaN                            NaN   \n",
       "1                             NaN                            NaN   \n",
       "2                             NaN                            NaN   \n",
       "3                             NaN                            NaN   \n",
       "4                             NaN                            NaN   \n",
       "\n",
       "   mobility_transit_stations  mobility_workplaces  mobility_residential  ...  \\\n",
       "0                        NaN                  NaN                   NaN  ...   \n",
       "1                        NaN                  NaN                   NaN  ...   \n",
       "2                        NaN                  NaN                   NaN  ...   \n",
       "3                        NaN                  NaN                   NaN  ...   \n",
       "4                        NaN                  NaN                   NaN  ...   \n",
       "\n",
       "   relative_humidity  new_confirmed_male  new_confirmed_female  \\\n",
       "0          72.773048                 NaN                   NaN   \n",
       "1          70.841316                 NaN                   NaN   \n",
       "2          71.117249                 NaN                   NaN   \n",
       "3          77.338638                 NaN                   NaN   \n",
       "4          60.762379                 NaN                   NaN   \n",
       "\n",
       "   cumulative_confirmed_male  cumulative_confirmed_female  country_code  \\\n",
       "0                        NaN                          NaN            AD   \n",
       "1                        NaN                          NaN            AD   \n",
       "2                        NaN                          NaN            AD   \n",
       "3                        NaN                          NaN            AD   \n",
       "4                        NaN                          NaN            AD   \n",
       "\n",
       "   country_name subregion1_name subregion2_name aggregation_level  \n",
       "0       Andorra            None            None               0.0  \n",
       "1       Andorra            None            None               0.0  \n",
       "2       Andorra            None            None               0.0  \n",
       "3       Andorra            None            None               0.0  \n",
       "4       Andorra            None            None               0.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop near-empty columns\n",
    "threshold = 0.8  # keep only columns with <80% missing\n",
    "df_reduced = df[df.columns[df.isna().mean() < threshold]]\n",
    "\n",
    "print(\"Reduced shape:\", df_reduced.shape)\n",
    "df_reduced.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a71067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.to_parquet(\"C:/Users/Deepak/OneDrive/Desktop/ObjectRegon/data/processed/merged_reduced.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Regon",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
